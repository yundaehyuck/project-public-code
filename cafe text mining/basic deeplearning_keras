#deeplearning

#keras를 이용

#필요한 라이브러리를 불러옵니다
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import urllib.request
from keras import models #케라스 라이브러리 불러오기
from keras import layers

from tensorflow.keras.preprocessing.text import Tokenizer #단어 벡터화 라이브러리 
from tensorflow.keras.preprocessing.sequence import pad_sequences #단어 벡터 패딩하는 라이브러리

#데이터 프레임 생성
data={'제목':title_copy,'종목':title_copy_tag_dummy}
df=pd.DataFrame(data)

#기술통계분석

#범주별 빈도수를 확인
df['종목'].value_counts().plot(kind='bar') #그래프
print(df.groupby('종목').size().reset_index(name='count')) #빈도표

#데이터를 독립변수와 종속변수로 구분
X_data = df['제목']
y_data = df['종목']

#데이터 단어 벡터화 실시
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_data) # 각 텍스트 데이터의 벡터화를 수행
sequences = tokenizer.texts_to_sequences(X_data) # 벡터화된 단어를 대응되는 인덱스로 변환하여 저장

#단어의 인덱스 mapping 사전을 출력
#해당 단어의 대응되는 인덱스를 확인할 수 있음
#빈도수가 높을수록 낮은 값의 인덱스가 대응
word_to_index = tokenizer.word_index
print(word_to_index)

#데이터에 존재하는 모든 단어 집합의 크기 계산
vocab_size = len(word_to_index) + 1
print('단어 집합의 크기: {}'.format((vocab_size)))

#데이터를 훈련 데이터와 시험 데이터로 나누는 작업
#랜덤으로 나눌수 있지만 편의상 
n_of_train = int(len(sequences) * 0.8)
n_of_test = int(len(sequences) - n_of_train)

X_data=sequences
X_test = X_data[n_of_train:] 
y_test = np.array(y_data[n_of_train:]) 
X_train = X_data[:n_of_train] 
y_train = np.array(y_data[:n_of_train])

print(X_train) #훈련데이터를 확인하면 각 데이터의 차원이 제각각임
#그래서 모든 데이터의 차원이 동일하게 차원이 빈 부분은 0을 넣어 모든 데이터의 차원을 100차원으로 만드는 패딩 작업을 실시
max_len = 100
X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩
X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩

#범주 데이터를 5차원으로 one hot encoding 실시
#0은 [1,0,0,0,0]
#1은 [0,1,0,0,0]
#2는 [0,0,1,0,0]
#3은 [0,0,0,1,0]
#4는 [0,0,0,0,1]
y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩
y_test = to_categorical(y_test)


#lstm 모델 적합

#필요한 라이브러리

from tensorflow.keras.datasets import reuters
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

#lstm 모델 생성
model = Sequential()
model.add(Embedding(vocab_size, 120))
model.add(LSTM(120))
model.add(Dense(5, activation='softmax')) #구분되는 범주가 5개이기 때문에 5로

#모델 훈련의 최상의 포인트를 찾기 위함
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) #과적합을 방지하기 위한 손실함수 계산
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) #최적 모델을 저장해줌

#손실함수와 최적값 계산하는 방식을 지정
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

#모델 훈련 실시
history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))

#훈련 상태에 최적 모델이 무엇인지 알려주는데 최적모델을 이용하여 정확도 계산
loaded_model = load_model('best_model.h5')
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))

#손실함수가 어떻게 떨어지는지 시각화하여 확인함
epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#새로운 데이터 생성
new_data=['토트넘은 지난 해 사우디 왕실에게 매각될 뻔 했다','담원 롤드컵 결승 오프더레코드','페이커 미친 무빙','NBA 스토브리그 근황','올해 일본 프로야구 일본시리즈에서 나온 비매너 플레이']
newtokenizer = Tokenizer()
newtokenizer.fit_on_texts(new_data)
newsequences = tokenizer.texts_to_sequences(new_data)
new_x = pad_sequences(newsequences, maxlen=max_len)

#새로운 데이터를 넣어 예측해봄
#예측이 정확하지 않아 과적합 의심
#사용되는 코드도 어려워 딥러닝에 대한 섬세한 공부가 필요할듯
model.predict(new_x)
