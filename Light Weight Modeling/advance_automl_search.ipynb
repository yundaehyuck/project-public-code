{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6d10eb-edc2-403d-a1ff-9f558bd113fe",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76049b19-fbe8-43f3-bada-2676f0b0b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Tuple, Union, List\n",
    "from tqdm import tqdm,notebook\n",
    "\n",
    "#automl optuna\n",
    "import optuna\n",
    "\n",
    "#sklearn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "\n",
    "#special optimizer\n",
    "\n",
    "import madgrad\n",
    "from adamp import SGDP,AdamP\n",
    "\n",
    "#baseline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('/opt/ml/code/src/'))))\n",
    "\n",
    "from src.dataloader import create_dataloader\n",
    "from src.loss import CustomCriterion\n",
    "from src.model import Model\n",
    "from src.trainer import TorchTrainer\n",
    "from src.utils.common import get_label_counts, read_yaml\n",
    "from src.utils.macs import calc_macs\n",
    "from src.utils.torch_utils import check_runtime, model_info\n",
    "from src.augmentation.policies import simple_augment_test\n",
    "from src.utils.inference_utils import run_model\n",
    "from src.utils.torch_utils import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6d1b6-1f2a-4ad6-9b34-dc471bf7d6e8",
   "metadata": {},
   "source": [
    "## hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef38a1e-0cf6-4763-8dbb-4b9149f2af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511862c4-7470-42be-a79f-19d48084adc4",
   "metadata": {},
   "source": [
    "## fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bd744a-6aed-4cb4-be0a-9f53a90b4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch seed\n",
    "torch.manual_seed(30)\n",
    "torch.cuda.manual_seed(30)\n",
    "\n",
    "#numpy seed\n",
    "np.random.seed(30)\n",
    "\n",
    "#python seed\n",
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775181a-6da7-4de1-b387-aa4632fec25c",
   "metadata": {},
   "source": [
    "## load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb33eb5-baed-4f37-b674-30282ebaf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CONFIG = read_yaml(cfg=\"/opt/ml/code/configs/data/taco.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd48072-4631-469e-b717-5babae3e6be8",
   "metadata": {},
   "source": [
    "## define hyperparameter search space function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5eb389-23aa-43aa-8ab8-f8b0488d85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hyperparam(trial: optuna.trial.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Search hyperparam from user-specified search space.\"\"\"\n",
    "    epochs = trial.suggest_int(\"epochs\", low=50, high=200, step=50)\n",
    "    img_size = trial.suggest_categorical(\"img_size\", [18,24,30,36,42])\n",
    "    #epochs = 200\n",
    "    #img_size = 96\n",
    "    n_select = trial.suggest_int(\"n_select\", low=0, high=6, step=2)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", low=16, high=64, step=16)\n",
    "    return {\n",
    "        \"EPOCHS\": epochs,\n",
    "        \"IMG_SIZE\": img_size,\n",
    "        \"n_select\": n_select,\n",
    "        \"BATCH_SIZE\": batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc703bd0-9ef3-4387-8400-3d26b6622270",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0a5ad9a-a9c6-4f81-af89-738c9b975bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = models.shufflenet_v2_x0_5(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbedbae7-17a2-4c74-b598-ab6e71cf917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.fc = nn.Linear(in_features=1024, out_features=9, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43e5aa85-71fc-4ad2-87c0-9bed9e2a417d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc254eab-af9c-4dc5-9dc3-f6e85326cfaa",
   "metadata": {},
   "source": [
    "## define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d26b9e6-77f2-46ac-83ae-7df18456a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_instance, model_path, optimizer, scheduler, criterion, scaler, n_epoch, train_dl, val_dl, device):\n",
    "    \n",
    "    #n_epoch = 30\n",
    "\n",
    "    best_test_acc = -1.0\n",
    "    best_test_f1 = -1.0\n",
    "\n",
    "    num_classes = 9\n",
    "\n",
    "    label_list = [i for i in range(num_classes)]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        ############## prune trial ###############\n",
    "        if epoch == 10:\n",
    "            if best_test_f1 < 0.28:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 10 epochs')\n",
    "                break\n",
    "        if epoch == 30:\n",
    "            if best_test_f1 < 0.4:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 30 epochs')\n",
    "                break\n",
    "        if epoch == 80:\n",
    "            if best_test_f1 < 0.5:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 80 epochs')\n",
    "                break\n",
    "        #############################################\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        preds, gt = [], []\n",
    "        pbar = notebook.tqdm(enumerate(train_dl), total=len(train_dl))\n",
    "        model_instance.train()\n",
    "        for batch, (data, labels) in pbar:\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model_instance(data)\n",
    "            else:\n",
    "                outputs = model_instance(data)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            preds += pred.to(\"cpu\").tolist()\n",
    "            gt += labels.to(\"cpu\").tolist()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.update()\n",
    "            pbar.set_description(\n",
    "                f\"Train: [{epoch + 1:03d}] \"\n",
    "                f\"Loss: {(running_loss / (batch + 1)):.3f}, \"\n",
    "                f\"Acc: {(correct / total) * 100:.2f}% \"\n",
    "                f\"F1(macro): {f1_score(y_true=gt, y_pred=preds, labels=label_list, average='macro', zero_division=0):.2f}\"\n",
    "            )\n",
    "        pbar.close()\n",
    "\n",
    "        _, test_f1, test_acc = test(\n",
    "            model=model_instance, test_dataloader=val_dl, scaler=scaler, criterion=criterion\n",
    "        )\n",
    "        if best_test_f1 > test_f1:\n",
    "            continue\n",
    "        best_test_acc = test_acc\n",
    "        best_test_f1 = test_f1\n",
    "        print(f\"Model saved. Current best test f1: {best_test_f1:.3f}\")\n",
    "        save_model(\n",
    "            model=model_instance,\n",
    "            path=model_path,\n",
    "            data=data,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    return best_test_acc,best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99f0429e-a487-47c2-b8c7-7e877960c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model,test_dataloader,scaler,criterion):\n",
    "    \"\"\"Test model.\n",
    "\n",
    "    Args:\n",
    "        test_dataloader: test data loader module which is a iterator that returns (data, labels)\n",
    "\n",
    "    Returns:\n",
    "        loss, f1, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #n_batch = _get_n_batch_from_dataloader(test_dataloader)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    preds = []\n",
    "    gt = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    num_classes = 9\n",
    "    label_list = [i for i in range(num_classes)]\n",
    "\n",
    "    pbar = notebook.tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for batch, (data, labels) in pbar:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(data)\n",
    "        else:\n",
    "            outputs = model(data)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        running_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "\n",
    "        preds += pred.to(\"cpu\").tolist()\n",
    "        gt += labels.to(\"cpu\").tolist()\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f\" Val: {'':5} Loss: {(running_loss / (batch + 1)):.3f}, \"\n",
    "            f\"Acc: {(correct / total) * 100:.2f}% \"\n",
    "            f\"F1(macro): {f1_score(y_true=gt, y_pred=preds, labels=label_list, average='macro', zero_division=0):.2f}\"\n",
    "        )\n",
    "    loss = running_loss / len(test_dataloader)\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(\n",
    "        y_true=gt, y_pred=preds, labels=label_list, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    \n",
    "    return loss, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662be91-b2e1-4a3a-a892-450033dfabe3",
   "metadata": {},
   "source": [
    "## define mixup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "689d411e-6dab-4307-a4f9-7844fba3da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3aebb2a-6523-45ef-968d-4b7125db3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_train(model_instance, model_path, optimizer, scheduler, criterion, scaler, n_epoch, train_dl, val_dl, device):\n",
    "    \n",
    "    #n_epoch = 30\n",
    "\n",
    "    best_test_acc = -1.0\n",
    "    best_test_f1 = -1.0\n",
    "\n",
    "    num_classes = 9\n",
    "\n",
    "    label_list = [i for i in range(num_classes)]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        ############## prune trial ###############\n",
    "        if epoch == 10:\n",
    "            if best_test_f1 < 0.28:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 10 epochs')\n",
    "                break\n",
    "        if epoch == 30:\n",
    "            if best_test_f1 < 0.4:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 30 epochs')\n",
    "                break\n",
    "        if epoch == 80:\n",
    "            if best_test_f1 < 0.5:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 80 epochs')\n",
    "                break\n",
    "        #############################################\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        preds, gt = [], []\n",
    "        pbar = notebook.tqdm(enumerate(train_dl), total=len(train_dl))\n",
    "        model_instance.train()\n",
    "        for batch, (data, labels) in pbar:\n",
    "\n",
    "            data,label_a,label_b,lam = mixup_data(data,labels,alpha=1.0)\n",
    "        \n",
    "            data, labels, label_a, label_b = data.to(device), labels.to(device), label_a.to(device), label_b.to(device)\n",
    "\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model_instance(data)\n",
    "            else:\n",
    "                outputs = model_instance(data)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            #loss = criterion(outputs, labels)\n",
    "            loss = criterion(outputs, label_a) * lam + criterion(outputs, label_b)* (1-lam)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            preds += pred.to(\"cpu\").tolist()\n",
    "            gt += labels.to(\"cpu\").tolist()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.update()\n",
    "            pbar.set_description(\n",
    "                f\"Train: [{epoch + 1:03d}] \"\n",
    "                f\"Loss: {(running_loss / (batch + 1)):.3f}, \"\n",
    "                f\"Acc: {(correct / total) * 100:.2f}% \"\n",
    "                f\"F1(macro): {f1_score(y_true=gt, y_pred=preds, labels=label_list, average='macro', zero_division=0):.2f}\"\n",
    "            )\n",
    "        pbar.close()\n",
    "\n",
    "        _, test_f1, test_acc = test(\n",
    "            model=model_instance, test_dataloader=val_dl, scaler=scaler, criterion=criterion\n",
    "        )\n",
    "        if best_test_f1 > test_f1:\n",
    "            continue\n",
    "        best_test_acc = test_acc\n",
    "        best_test_f1 = test_f1\n",
    "        print(f\"Model saved. Current best test f1: {best_test_f1:.3f}\")\n",
    "        save_model(\n",
    "            model=model_instance,\n",
    "            path=model_path,\n",
    "            data=data,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    return best_test_acc,best_test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681bb11-6749-4d6f-bfec-4b3cc1759e56",
   "metadata": {},
   "source": [
    "## define cutmix training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85d7be96-9d70-4b7a-b440-c04fbef83571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "062c75af-36f2-49ae-9a2a-463a15ec3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CutMix(input,target,cutmix_prob,beta=1.0):\n",
    "    r = np.random.rand(1)\n",
    "    if beta > 0 and r < cutmix_prob:\n",
    "        lam = np.random.beta(beta, beta)\n",
    "        rand_index = torch.randperm(input.size()[0]).cuda()\n",
    "        target_a = target\n",
    "        target_b = target[rand_index]\n",
    "        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
    "        input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "        return input,lam,target_a,target_b,True\n",
    "    else:\n",
    "        lam = np.random.beta(beta, beta)\n",
    "        return input,lam,target,target,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "232b2fed-6a9e-4542-8261-26b134274817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_train(model_instance, model_path, optimizer, scheduler, criterion, scaler, n_epoch, train_dl, val_dl, device):\n",
    "    \n",
    "    #n_epoch = 30\n",
    "\n",
    "    best_test_acc = -1.0\n",
    "    best_test_f1 = -1.0\n",
    "\n",
    "    num_classes = 9\n",
    "\n",
    "    label_list = [i for i in range(num_classes)]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        ############## prune trial ###############\n",
    "        if epoch == 10:\n",
    "            if best_test_f1 < 0.28:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 10 epochs')\n",
    "                break\n",
    "        if epoch == 30:\n",
    "            if best_test_f1 < 0.4:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 30 epochs')\n",
    "                break\n",
    "        if epoch == 80:\n",
    "            if best_test_f1 < 0.5:\n",
    "                print(f'This model has very low f1:{best_test_f1} in 80 epochs')\n",
    "                break\n",
    "        #############################################\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        preds, gt = [], []\n",
    "        pbar = notebook.tqdm(enumerate(train_dl), total=len(train_dl))\n",
    "        model_instance.train()\n",
    "        for batch, (data, labels) in pbar:\n",
    "            \n",
    "            data,lam,target_a,target_b,cut = CutMix(data,labels,cutmix_prob=0.5)\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model_instance(data)\n",
    "            else:\n",
    "                outputs = model_instance(data)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            #loss = criterion(outputs, labels)\n",
    "            \n",
    "            if cut:\n",
    "                target_a = target_a.to(device)\n",
    "                target_b = target_b.to(device)\n",
    "                loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b)* (1-lam)\n",
    "            else:\n",
    "                target_a = target_a.to(device)\n",
    "                loss = criterion(outputs, target_a)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            preds += pred.to(\"cpu\").tolist()\n",
    "            gt += labels.to(\"cpu\").tolist()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.update()\n",
    "            pbar.set_description(\n",
    "                f\"Train: [{epoch + 1:03d}] \"\n",
    "                f\"Loss: {(running_loss / (batch + 1)):.3f}, \"\n",
    "                f\"Acc: {(correct / total) * 100:.2f}% \"\n",
    "                f\"F1(macro): {f1_score(y_true=gt, y_pred=preds, labels=label_list, average='macro', zero_division=0):.2f}\"\n",
    "            )\n",
    "        pbar.close()\n",
    "\n",
    "        _, test_f1, test_acc = test(\n",
    "            model=model_instance, test_dataloader=val_dl, scaler=scaler, criterion=criterion\n",
    "        )\n",
    "        if best_test_f1 > test_f1:\n",
    "            continue\n",
    "        best_test_acc = test_acc\n",
    "        best_test_f1 = test_f1\n",
    "        print(f\"Model saved. Current best test f1: {best_test_f1:.3f}\")\n",
    "        save_model(\n",
    "            model=model_instance,\n",
    "            path=model_path,\n",
    "            data=data,\n",
    "            device=device,\n",
    "        )\n",
    "    \n",
    "    return best_test_acc,best_test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99356f-dbb0-4c71-88ab-0a9db6fb7d89",
   "metadata": {},
   "source": [
    "## define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ff995e2-ce6e-4817-81c7-f259fc4810a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, device) -> Tuple[float, int, float]:\n",
    "    \n",
    "    global before_macs,success\n",
    "    \n",
    "    data_config = copy.deepcopy(DATA_CONFIG)\n",
    "    model_instance = models.shufflenet_v2_x0_5(pretrained=True)\n",
    "    model_instance.fc = nn.Linear(in_features=1024, out_features=9, bias=True)\n",
    "\n",
    "    # hyperparams: EPOCHS, IMG_SIZE, n_select, BATCH_SIZE\n",
    "    hyperparams = search_hyperparam(trial)\n",
    "\n",
    "    data_config[\"AUG_TRAIN_PARAMS\"][\"n_select\"] = hyperparams[\"n_select\"]\n",
    "    data_config[\"BATCH_SIZE\"] = hyperparams[\"BATCH_SIZE\"]\n",
    "    data_config[\"EPOCHS\"] = hyperparams[\"EPOCHS\"]\n",
    "    data_config[\"IMG_SIZE\"] = hyperparams[\"IMG_SIZE\"]\n",
    "\n",
    "    log_dir = os.path.join(\"exp\", datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    \n",
    "\n",
    "    macs = calc_macs(model_instance, (3, data_config[\"IMG_SIZE\"], data_config[\"IMG_SIZE\"]))\n",
    "    print(f\"macs: {macs}\")\n",
    "    \n",
    "    if trial.number > 1:\n",
    "        if success:\n",
    "            if macs > before_macs:\n",
    "                print(f' trial: {trial.number}, I want to search lower macs: now {macs} >>> before {before_macs}')\n",
    "                raise optuna.structs.TrialPruned()\n",
    "    \n",
    "    if macs>2500000.0:      ########\n",
    "        print(f' trial: {trial.number}, This model has very large macs:{macs}')\n",
    "        success = False\n",
    "        raise optuna.structs.TrialPruned()##############\n",
    "    \n",
    "    #training\n",
    "    \n",
    "    #fixed hyperparameter\n",
    "    \n",
    "    model_instance = torch.nn.DataParallel(model_instance) #half tensor를 float tensor로 바꿔줌\n",
    "    \n",
    "    train_path = os.path.join(data_config[\"DATA_PATH\"], \"train\")\n",
    "    \n",
    "    train_dl, val_dl, test_dl = create_dataloader(data_config)\n",
    "    \n",
    "    loss_type = trial.suggest_categorical('loss',['softmax','weighted','customloss','logit_adjustment_loss','label_smoothing'])\n",
    "    \n",
    "    criterion = CustomCriterion(\n",
    "        samples_per_cls=get_label_counts(train_path)\n",
    "        if data_config[\"DATASET\"] == \"TACO\"\n",
    "        else None,\n",
    "        device=device,\n",
    "        loss_type=loss_type\n",
    "    )\n",
    "    \n",
    "    # Amp loss scaler\n",
    "    scaler = (\n",
    "        torch.cuda.amp.GradScaler() if data_config['FP16'] and device != torch.device(\"cpu\") else None\n",
    "    )\n",
    "    #scaler=None\n",
    "    \n",
    "    lr = trial.suggest_uniform('learning_rate',0.1,1.0)\n",
    "    momentum = trial.suggest_uniform('momentum',0.0,1.0)\n",
    "    weight_decay = trial.suggest_uniform('weight_decay',1e-6,1e-1)\n",
    "    nesterov = trial.suggest_categorical('nesterov',[True,False])\n",
    "    \n",
    "    optimizer_type = trial.suggest_categorical('optimizer',['SGD','Adam','AdamP','SGDP','madgrad'])\n",
    "    \n",
    "    if optimizer_type == 'SGD':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model_instance.parameters(), lr=lr, momentum=momentum\n",
    "        )\n",
    "    elif optimizer_type == 'SGDP':\n",
    "        optimizer = SGDP(model_instance.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model_instance.parameters(), betas=(0.9,0.999), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'AdamP':\n",
    "        optimizer = AdamP(model_instance.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer= madgrad.MADGRAD(model_instance.parameters(),lr=lr,momentum=momentum)\n",
    "\n",
    "    \n",
    "    gamma = trial.suggest_uniform('gamma',0.4,0.8)\n",
    "    \n",
    "    scheduler_type = trial.suggest_categorical('scheduler',['OneCycleLR','MultistepLR'])\n",
    "    \n",
    "    if scheduler_type == 'OneCycleLR':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=data_config[\"INIT_LR\"],\n",
    "            steps_per_epoch=len(train_dl),\n",
    "            epochs=data_config[\"EPOCHS\"],\n",
    "            pct_start=0.05,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=list(range(10,data_config['EPOCHS'],10)), gamma=gamma)\n",
    "    \n",
    "\n",
    "    save_path = os.path.join(log_dir, \"best.pt\")\n",
    "    \n",
    "    print(f\"train {data_config['EPOCHS']} epochs this model!\")\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(log_dir, 'data.yml'), 'w') as f:\n",
    "        yaml.dump(data_config, f, default_flow_style=False)\n",
    "        \n",
    "    \n",
    "    training_type = trial.suggest_categorical('training',['basic','mixup','cutmix'])\n",
    "    \n",
    "    if training_type == 'basic':\n",
    "        _, best_f1 = train(\n",
    "            model_instance,\n",
    "            save_path,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            criterion, \n",
    "            scaler,\n",
    "            data_config[\"EPOCHS\"],\n",
    "            train_dl, \n",
    "            val_dl, \n",
    "            device\n",
    "        )\n",
    "    elif training_type == 'mixup':\n",
    "        \n",
    "        criterion = CustomCriterion(\n",
    "        samples_per_cls=get_label_counts(train_path)\n",
    "        if data_config[\"DATASET\"] == \"TACO\"\n",
    "        else None,\n",
    "        device=device,\n",
    "        loss_type='label_smoothing')\n",
    "        \n",
    "        _, best_f1 = mixup_train(\n",
    "            model_instance,\n",
    "            save_path,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            criterion, \n",
    "            scaler,\n",
    "            data_config[\"EPOCHS\"],\n",
    "            train_dl, \n",
    "            val_dl, \n",
    "            device\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        criterion = CustomCriterion(\n",
    "        samples_per_cls=get_label_counts(train_path)\n",
    "        if data_config[\"DATASET\"] == \"TACO\"\n",
    "        else None,\n",
    "        device=device,\n",
    "        loss_type='label_smoothing')\n",
    "        \n",
    "        _, best_f1 = cutmix_train(\n",
    "            model_instance,\n",
    "            save_path,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            criterion, \n",
    "            scaler,\n",
    "            data_config[\"EPOCHS\"],\n",
    "            train_dl, \n",
    "            val_dl, \n",
    "            device\n",
    "        )\n",
    "        \n",
    "    \n",
    "    before_macs = macs\n",
    "    success = True\n",
    "\n",
    "    return best_f1, macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab04934b-944b-496d-b4d2-d71477f581e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(device, study_name= \"pstage_automl\"):\n",
    "\n",
    "    sampler = optuna.samplers.MOTPESampler(n_startup_trials=20)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        directions=[\"maximize\", \"minimize\"],\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    study.optimize(lambda trial: objective(trial, device), n_trials=100)\n",
    "\n",
    "    pruned_trials = [\n",
    "        t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED\n",
    "    ]\n",
    "    complete_trials = [\n",
    "        t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE\n",
    "    ]\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trials:\")\n",
    "    best_trials = study.best_trials\n",
    "\n",
    "    ## trials that satisfies Pareto Fronts\n",
    "    for tr in best_trials:\n",
    "        print(f\"  value1:{tr.values[0]}, value2:{tr.values[1]}\")\n",
    "        for key, value in tr.params.items():\n",
    "            print(f\"    {key}:{value}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe94d-a6c4-4708-a202-f52770019bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-27 23:19:57,619]\u001b[0m A new study created in memory with name: pstage_automl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macs: 2100689.0\n",
      "train 150 epochs this model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085a916107f14ae99f1a465b4ba8258f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=764.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "study_name=\"pstage_automl\"\n",
    "\n",
    "study = tune(device, study_name=study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7ad23-f5b3-4bcd-ab86-1026762f4677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
